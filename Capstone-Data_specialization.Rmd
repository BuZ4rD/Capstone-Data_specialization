---
title: "EDA Swiftkey - Capstone Project"
author: "Mathias Barat"
date: "21/09/2020"
output: html_document
---
### Task1 and Task2 of the Data Science Capstone from JHU

# 1 - SYNOPSIS

Swiftkey dataset. The analysis and goal is to help Swiftkey company to predict words when users type on their keyboard.
The prediction must be done from 1, 2 or 3 successive words entered.

The present document is:
  * giving an overview of the dataset used
  * taking assumptions regarding the sampling approach as the dataset is quite big
  * making an EDA
  * giving some clues for the predictive model I will create

```{r 1, message=FALSE, warning=FALSE}
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
```

# 2 - DATA-PREPROCESSING

### GET THE DATASET
Download:
```{r 2, message=FALSE, warning=FALSE}

if (!dir.exists("data/")){dir.create("data/")}
if (!file.exists("data/Cousera-swiftkey.zip")){
  download.file(url, destfile = "data/Cousera-swiftkey.zip")
}
```
Unzip:
```{r 3, message=FALSE, warning=FALSE}
if (!dir.exists("final/")){
  unzip("data/Cousera-swiftkey.zip")
}
```

### OVERVIEW OF THE DATASET
The dataset is from a corpus called HC Corpora collected by a web crawler.

The dataset is composed of 4 folders :
  
  * de_DE : German language text files
  * en_US : English language text files
  * fi_FI : Finns language text files
  * ru_RU : Russian language text files

In each folder we can find 3 files corresponding to compilation of web published texts which have been gathered from :
  * Blogs websites
  * News website
  * Twitter
  
Only english files will be used in the following study.

### Natural Language Processing (NLP)
The Natural language processing is a domain of data science developped to analyse the human languages.
    * https://en.wikipedia.org/wiki/Natural_language_processing
    
Tools such as Text Mining (tm package in R) has been developped to process the human text data.
  * https://cran.r-project.org/web/views/NaturalLanguageProcessing.html
  
Machine learning methods are now a standard in the domain.The present analysis will use it.

The NLP can helps to analyse the languages for different purposes such as :

Text and speech processing
  * Optical character recognition (OCR)
  * Speech recognition
  * Speech segmentation
  * Text-to-speech
  * Word segmentation (Tokenization)

Morphological analysis
  * Lemmatization
  * Morphological segmentation
  * Part-of-speech tagging
  * Stemming

Syntactic analysis
  * Grammar induction
  * Sentence breaking (also known as "sentence boundary disambiguation")
  * Parsing
  
Lexical semantics (of individual words in context)
  * Lexical semantics
  * Distributional semantics
  * Named entity recognition (NER).
  * Sentiment analysis (see also multimodal sentiment analysis).
  * Terminology extraction
  * Word sense disambiguation

Relational Semantics
  * Relationship extraction
  * Semantic Parsing
  * Semantic Role Labelling (see also implicit semantic role labelling below)

Discourse (semantics beyond individual sentences)
  * Coreference resolution
  * Discourse analysis
  * Implicit Semantic Role Labelling
  * Recognizing Textual entailment

Higher-level NLP applications
  * Book generation
  * Machine translation
  * Natural language understanding (NLU)
  * Question answering


The training I had during the specialization will help me with :
  * Preprocessing of the data : preparation of the dataset to be usable by ML and associated Data science tools (split, case, cleaning, removing )
  * Analysis of the data : vectorization, bagging...
  * Exploitation of the results
  
I will use also other data sources such as :
  * Other NLP processing as example on Kaggle for instance
  * Find datasets and their reproducible analysis in each language that can help me with the specificities of each.
  * wordnet and similar databases
  * list of profanes words

### Loading the data
```{r 4, message=FALSE, warning=FALSE}

# The 2 files are okay to be opened directly with UTF-8
con <- file("final/en_US/en_US.blogs.txt", "r")
dblog <- readLines(con, encoding = "UTF-8", skipNul = TRUE, warn = FALSE)
close(con)

con <- file("final/en_US/en_US.twitter.txt","r")
dtwit <- readLines(con, encoding = "UTF-8", skipNul = TRUE, warn = FALSE)
close(con)

# Binary mode because the file has special characters, the readlines method was blocked with the previous method
con <- file("final/en_US/en_US.news.txt", open="rb")
dnews <- readLines(con, encoding = "UTF-8",skipNul = TRUE, warn = FALSE)
close(con)
```

### sizes and format of the dataset
```{r 5, message=FALSE, warning=FALSE}

dat_sum <- file.info(dir("final/en_US/",recursive = TRUE, full.names = TRUE))[1]/1024^2
colnames(dat_sum) <- "Size_in_Mo"
dat_sum$Number_Of_Lines <- c(length(dblog),length(dnews),length(dtwit))
dat_sum$Number_of_Words <- c( sum(sapply(gregexpr("\\S+", dblog), length)),
                    sum(sapply(gregexpr("\\S+", dnews), length)),
                    sum(sapply(gregexpr("\\S+", dtwit), length)))

dat_sum$Mean_words_per_line <- c(mean(sapply(gregexpr("\\S+", dblog), length)),
                        mean(sapply(gregexpr("\\S+", dnews), length)),
                        mean(sapply(gregexpr("\\S+", dtwit), length)))

dat_sum$Max_Line <- sapply(list(dblog, dnews, dtwit), function(x){max(unlist(lapply(x, function(y) nchar(y))))})

dat_sum$File <- c("Blogs",
                  "News",
                  "Twitter")

dat_sum <- as.data.frame(dat_sum)
```


### Sampling
I will sample 5% of the Blogs data, 10% of the News data and 5% of the Twitter data:
```{r 6, message=FALSE, warning=FALSE}
dat_sum$Percentage <- c(0.05,0.1,0.05)
dat_sum$Sample_size <- dat_sum$Number_Of_Lines* dat_sum$Percentage
set.seed(333)
dblogS  <- dblog[rbinom(n = dat_sum$Sample_size[1],
                        size = 1,
                        prob = dat_sum$Percentage[1]) == 1]

dnewsS <- dnews[rbinom( n = dat_sum$Sample_size[2],
                        size = 1,
                        prob = dat_sum$Percentage[2]) == 1]

dtwitS <- dtwit[rbinom( n = dat_sum$Sample_size[3],
                        size = 1,
                        prob = dat_sum$Percentage[3]) == 1]

# Creation of one file
dataSample <- c(dblogS,dnewsS, dtwitS)

# clean non latin characters
dataSample <- iconv(dataSample, "latin1", "ASCII", sub="")

# Save my sample file
write(dataSample, "data/dataSample.txt")
rm(dblog, dnews, dtwit, dblogS, dnewsS, dtwitS, con, dataSample)
```



### Cleaning and Profanes words removal
```{r 7, message=FALSE, warning=FALSE}
library(tm)
dataSample <- readLines("data/dataSample.txt")
#dataSample <- sample(dataSample, 1000) # LINE TO REMOVE / Siplification for test
CorpusSample = VCorpus(VectorSource(dataSample))  
rm(dataSample)

# Get list of bad words to be excluded
url_EN_prof = "https://raw.githubusercontent.com/rominf/profanity-filter/master/profanity_filter/data/en_profane_words.txt"
if (!file.exists("data/prof_en.txt")){
  download.file(url_EN_prof, destfile = "data/prof_en.txt")
}
con <- file("data/prof_en.txt")
prof_en <- readLines(con)
names(prof_en)<-"Bad Words"
close(con)
rm(con, url_EN_prof)

# Cleaning:
CorpusSample <- tm_map(CorpusSample, tolower)
CorpusSample <- tm_map(CorpusSample, removePunctuation)
CorpusSample <- tm_map(CorpusSample, removeNumbers)

# Pattern suppressing (emails, urls...)
url_rem <-function(x) gsub("(ftp|http)(s?)://.*\\b", "", x)
CorpusSample<-tm_map(CorpusSample, url_rem)

twitter_rem <- function(x) gsub("[@][a-zA-Z]{1,25}", "", x)
CorpusSample<-tm_map(CorpusSample, twitter_rem)

email_rem <- function(x) gsub("\\b[A-Z a-z 0-9._ - ]*[@](.*?)[.]{1,3} \\b", "", x)
CorpusSample<-tm_map(CorpusSample, email_rem)

# Profanities
CorpusSample <- tm_map(CorpusSample, removeWords, prof_en)

# Stopword
CorpusSampleSW <- tm_map(CorpusSample, removeWords, stopwords("english"))

# Whitespaces removal
CorpusSample <- tm_map(CorpusSample, stripWhitespace)
write(as.character(CorpusSample), "data/dataSampleClean.txt")
write(as.character(CorpusSampleSW), "data/dataSampleCleanStop.txt")
rm(list=ls())
```

# 3 - Exploratory Data Analysis

## Distribution of words
### Ngrams with stopwords
```{r 8, message=FALSE, warning=FALSE}
library(tm)
library(stylo)

CorpusSampCl <- readLines("data/dataSampleClean.txt")
CorpusSampCl<-  txt.to.words(CorpusSampCl)

ngram1 <- data.frame(table(make.ngrams(CorpusSampCl, ngram.size = 1)))
write.table(ngram1, "data/ngram1.txt")
ngram1 <- ngram1[order(ngram1$Freq,decreasing = TRUE)[1:20],]

ngram2 <- data.frame(table(make.ngrams(CorpusSampCl, ngram.size = 2)))
write.table(ngram2, "data/ngram2.txt")
ngram2 <- ngram2[order(ngram2$Freq,decreasing = TRUE)[1:20],]

ngram3 <- data.frame(table(make.ngrams(CorpusSampCl, ngram.size = 3)))
write.table(ngram3, "data/ngram3.txt")
ngram3 <- ngram3[order(ngram3$Freq,decreasing = TRUE)[1:20],]

ngram1$Type <- "Unigram"
ngram2$Type <- "Bigram"
ngram3$Type <- "Trigram"

# Bind grams for plot
ngram <- rbind(ngram1, ngram2, ngram3)
colnames(ngram) <- list("Words","Frequency","Type")
write.table(ngram, "data/ngram.txt")
rm(list=ls())
```

### Plot Unigram
```{r 9, message=FALSE, warning=FALSE}
library(ggplot2)
ngram <- read.table("data/ngram.txt")
g <- ggplot(data=ngram[ngram$Type=="Unigram",], aes(y=reorder(Words, Frequency, decreasing=FALSE), x = Frequency))+
          geom_bar( stat = "Identity", fill="yellow" ) +
          geom_text( aes (label = Frequency ) , vjust = - 0.20, size = 3 ) +
          ylab("Unigram") +
          xlab( "Frequency" )
g
```

### Plot Bigram
```{r 10, message=FALSE, warning=FALSE}
g <- ggplot(data=ngram[ngram$Type=="Digram",], aes(y=reorder(Words, Frequency, decreasing=FALSE), x = Frequency))+
          geom_bar( stat = "Identity", fill="blue" ) +
          geom_text( aes (label = Frequency ) , vjust = - 0.20, size = 3 ) +
          ylab("Digram") +
          xlab( "Frequency" )
g
```


### Plot Trigram
```{r 11, message=FALSE, warning=FALSE}
g <- ggplot(data=ngram[ngram$Type=="Trigram",], aes(y=reorder(Words, Frequency, decreasing=FALSE), x = Frequency))+
          geom_bar( stat = "Identity", fill="red" ) +
          geom_text( aes (label = Frequency ) , vjust = - 0.20, size = 3 ) +
          ylab("Trigram") +
          xlab( "Frequency" )
g
```

### Ngrams Analysis without stopwords
It might not be useful for the application we are developping, but I'll need clues regarding the frequencies without StopWords.
```{r 12, message=FALSE, warning=FALSE}
library(stylo)
library(tm)
CorpusSampCl <- readLines("data/dataSampleCleanStop.txt")
CorpusSampCl<-  txt.to.words(CorpusSampCl)

ngram1 <- data.frame(table(make.ngrams(CorpusSampCl, ngram.size = 1)))
ngram1 <- ngram1[order(ngram1$Freq,decreasing = TRUE)[1:20],]
ngram2 <- data.frame(table(make.ngrams(CorpusSampCl, ngram.size = 2)))
ngram2 <- ngram2[order(ngram2$Freq,decreasing = TRUE)[1:20],]
ngram3 <- data.frame(table(make.ngrams(CorpusSampCl, ngram.size = 3)))
ngram3 <- ngram3[order(ngram3$Freq,decreasing = TRUE)[1:20],]
ngram1$Type <- "Unigram"
ngram2$Type <- "Bigram"
ngram3$Type <- "Trigram"

# Bind grams for plot
ngram <- rbind(ngram1, ngram2, ngram3)
colnames(ngram) <- list("Words","Frequency","Type")
write.table(ngram, "data/ngramStop.txt")
rm(list=ls())
```

### Plot Unigram without stop words
```{r 13, message=FALSE, warning=FALSE}
library(ggplot2)
ngram <- read.table("data/ngramStop.txt")
g <- ggplot(data=ngram[ngram$Type=="Unigram",], aes(y=reorder(Words, Frequency, decreasing=FALSE), x = Frequency))+
          geom_bar( stat = "Identity", fill="yellow" ) +
          geom_text( aes (label = Frequency ) , vjust = - 0.20, size = 3 ) +
          ylab("Unigram") +
          xlab( "Frequency" )
        
            
g
```

### Plot Bigram without stop words
```{r 14, message=FALSE, warning=FALSE}
g <- ggplot(data=ngram[ngram$Type=="Bigram",], aes(y=reorder(Words, Frequency, decreasing=FALSE), x = Frequency))+
          geom_bar( stat = "Identity", fill="blue" ) +
          geom_text( aes (label = Frequency ) , vjust = - 0.20, size = 3 ) +
          ylab("Bigram") +
          xlab( "Frequency" )
g
```

### Plot Trigram without stop words
```{r 15, message=FALSE, warning=FALSE}
g <- ggplot(data=ngram[ngram$Type=="Trigram",], aes(y=reorder(Words, Frequency, decreasing=FALSE), x = Frequency))+
          geom_bar( stat = "Identity", fill="red" ) +
          geom_text( aes (label = Frequency ) , vjust = - 0.20, size = 3 ) +
          ylab("Trigram") +
          xlab( "Frequency" )
g
rm(list=ls())
```

## Number of words for 50% and 90% of the frequencies
```{r 16, message=FALSE, warning=FALSE}
ngram1 <- read.table("data/ngram1.txt")
ngram1 <- ngram1[order(ngram1$Freq,decreasing = TRUE),]

sumCover <- 0
sumNgram1 <- sum(ngram1$Freq)
t <- TRUE
for(i in 1:length(ngram1$Freq)) {
  sumCover <- sumCover + ngram1$Freq[i]
  if(sumCover >= 0.5*sumNgram1 & t){
    word50 <-i
    t<- FALSE
    }
  if(sumCover >= 0.9*sumNgram1){
    word90 <-i
    break}
}
" words to reach 50%"; word50
" words to reach 90%"; word90
```


## Foreign words
I switched to Quanteda, the library seems to be really more efficient than my previous methods...pretty cool
```{r 17, message=FALSE, warning=FALSE}
library(readtext)
library(quanteda)

Corpus <- readLines("data/dataSampleClean.txt")
Corpus <- corpus(Corpus)  

# tokenize the corpus
myTokens <- tokens(Corpus, remove_punct = TRUE, remove_numbers = TRUE)
# keep only the tokens found in an English dictionary
myTokens <- tokens_remove(myTokens, "a")

Foreign_dfm <- dfm(tokens_remove(myTokens, names(data_int_syllables)))

topfeatures(Foreign_dfm,30)
```
The "foreign words" are generally wrong writings... It might be taken in consideration during the model creation.

Just for fun a Wordl Cloud
```{r 18, message=FALSE, warning=FALSE}
textplot_wordcloud(Foreign_dfm, min_count = 6, random_order = FALSE,
                   rotation = .25, 
                   colors = RColorBrewer::brewer.pal(8,"Dark2"))
rm(list = ls())
```

### Reducing the number of words in the dictionary
A simple and quick solution would be to suppress all the low frequency words from the dictionary. For instance, all the 1 or 2 time

For instance here a list of 10 less used words (we can notice a famous League Of Legend character):
```{r 19, message=FALSE, warning=FALSE}
library(readtext)
library(quanteda)

Corpus <- readLines("data/dataSampleClean.txt")
Corpus = corpus(Corpus)  

# tokenize the corpus
myTokens <- tokens(Corpus, remove_punct = TRUE, remove_numbers = TRUE)
# keep only the tokens found in an English dictionary

Good_dfm <- dfm(tokens_keep(myTokens, names(data_int_syllables)),remove = stopwords("english"))

topfeatures(Good_dfm,10, decreasing = FALSE)

Light_dfm <- dfm_trim(
  Good_dfm,
  min_termfreq = 3
)
```
```{r 19, message=FALSE, warning=FALSE}
(nfeat(Good_dfm) - nfeat(Light_dfm))/ nfeat(Good_dfm) 


```
Just as an example, by suppressing the words which are appearing 3 times maximum, we can reduce by about 33% the dictionary size.
During the modeling, we will take that in account.

# 5 - Conclusions

This exercise taught me 2 things:
  * starting to understand the NLP and the libraries associated. I changed in the middle and I strated to use Quanteda as I felt it more efficient and easy to us (for now).
  * seeing that we **really** have to take care of the memory usage of the computer. When the dataset is getting bigger, the time to compute can be incredibly long and really pain in the ass. So finding the good mitigation for the sample size is important. I managed also to create a lot of "Checkpoints" in my pogram to clean the memory and start fresh with local files I am creating all along. 
